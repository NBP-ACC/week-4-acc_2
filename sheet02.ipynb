{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Osnabrück University - A&C: Computational Cognition (Summer Term 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Exercise Sheet 02: Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in at 14:00 at **Tuesday, April 30, 2019**. If you need help (and Google and other resources were not enough), feel free to contact your tutors. Please push your results to your Github group folder.\n",
    "\n",
    "In this exercise sheet you will have to work with ```pandas``` and ```seaborn```. ```pandas``` is one of the most preferred and widely used tools in data processing. What’s cool about ```pandas``` is that it takes data (like a CSV or TSV file, or a SQL database) and creates a Python object with rows and columns called 'data frame' that looks very similar to tables in a statistical software (think Excel or SPSS for example). ```pandas``` makes data processing a lot easier in comparison to working with lists and/or dictionaries through for-loops or list comprehension.  \n",
    "```seaborn``` is a library for making plots. It is based on ```matplotlib``` but offers more functions speicialized for statistical visualization. Also most people agree that ```seaborn``` looks more legit.\n",
    "\n",
    "Don't forget that you we will also give **2 points** for nice coding style!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Assignment 0: Peer review for sheet 01 [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Beginning this week you will have to make a peer review of the other groups' solutions. Each group reviews the solutions of two other groups and give points according to the given point distribution considering the correctness of the solution. For this reviews the tutors will give you up to 3 points each week.\n",
    "\n",
    "| * |Group 1|Group 2|Group 3|Group 4|Group 5|Group 6|Group 7|Group 8|Group 9|Group 10|Group 11|\n",
    "| ------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------ | ------ |\n",
    "| check solutions of group: | 10, 7 | 4, 9  | 1, 4  | 11, 1 | 8, 11 | 5, 3  | 9, 10 | 6, 5  | 3, 2  | 2, 8   | 7, 6   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "You should open an issue in repositories of groups you have to check. The title of the issue should be your group name (e.g.\"Group 1\"). Comments on what was good and bad, how much points they get etc.  \n",
    "Refer to https://guides.github.com/features/issues/ to learn more about issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Assignment 1: Dataframes [4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "```matplotlib``` and ```seaborn``` should already be installed in your environment. If not please run:\n",
    "```sh\n",
    "pip install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### a) Importing a csv file [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Import the csv files of all subjects into one dataframe. Make sure that each row has a unique index. You might want to take a look at what ***pandas.concat*** does.<br>\n",
    "Extra fun: Display the output of the dataframe using the ***pandas.set_option*** function to display the data in a well-arranged way. Play a little bit around with the settings that you are allowed to change.<br>\n",
    "Save ```df_concatenated```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SubjectID  StimulusType  response   RT\n",
      "0          111             1         0    0\n",
      "1          111             0         0    0\n",
      "2          111             1         1  585\n",
      "3          111             1         1  615\n",
      "4          111             1         1  402\n",
      "5          111             0         0    0\n",
      "6          111             1         1  719\n",
      "7          111             1         1  551\n",
      "8          111             1         1  482\n",
      "9          111             1         1  483\n",
      "10         111             0         0    0\n",
      "11         111             1         1  368\n",
      "12         111             1         1  446\n",
      "13         111             1         1  497\n",
      "14         111             1         1  530\n",
      "15         111             1         1  633\n",
      "16         111             1         1  448\n",
      "17         111             1         1  435\n",
      "18         111             1         1  385\n",
      "19         111             1         1  369\n",
      "..         ...           ...       ...  ...\n",
      "780        313             0         0    0\n",
      "781        313             1         1  451\n",
      "782        313             1         1  323\n",
      "783        313             1         1  290\n",
      "784        313             1         1  322\n",
      "785        313             0         0    0\n",
      "786        313             1         1  499\n",
      "787        313             1         1  323\n",
      "788        313             1         1  289\n",
      "789        313             1         1  291\n",
      "790        313             1         1  273\n",
      "791        313             0         0    0\n",
      "792        313             1         1  352\n",
      "793        313             1         1  323\n",
      "794        313             0         0    0\n",
      "795        313             1         1  306\n",
      "796        313             1         1  306\n",
      "797        313             1         1  321\n",
      "798        313             1         1  289\n",
      "799        313             1         1  355\n",
      "\n",
      "[800 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the new, object-orientated Python paths rather than the old modules\n",
    "# By this, we have both a convenient interface and no struggles with separators\n",
    "PATH = Path.cwd() / \"Data\"\n",
    "\n",
    "# Read all the experiments from the available CSV files and concatenate them.\n",
    "# In oder of getting not n time indeces from 0-99, we reset the indices by ignore_index.\n",
    "experiments = [pd.read_csv(file) for file in PATH.glob('*.csv')]\n",
    "df_concatenated = pd.concat(experiments, ignore_index=True)\n",
    "\n",
    "# Before showing an overview of the data, we optimize the formatting.\n",
    "# We justify the headers to the right and limit the maximal about of printed rows further.\n",
    "pd.set_option('colheader_justify', 'right')\n",
    "pd.set_option('display.max_rows', 40)\n",
    "print(df_concatenated)\n",
    "\n",
    "# In the nex step, we save the concatenated dataframe.\n",
    "# Therefore, we create the folder if it does not exist and (over-)write the CSV file.\n",
    "DATAPATH = Path.cwd() / 'Processed' / 'data_concatenated.csv'\n",
    "DATAPATH.parent.mkdir(exist_ok=True)\n",
    "with DATAPATH.open(mode='w', newline='') as file:\n",
    "    df_concatenated.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### b) Working with dataframes [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "- Add a column called \"congruence\" to ```df_concatenated```. The column should have a value *True* if \"StimulusType\" and \"reponse\" matches. Otherwise the column should have a value *False*.\n",
    "\n",
    "- Create a new dataframe which has \"SubjectID\",\"StiumulusType\",\"RT\" and \"congruence\" as a column. For each combination of \"SubjectID\" and \"StimulusType\" (e.g. \"7001\" and \"0\") compute the average RT and congruence level.\n",
    "\n",
    "- When computing the average RT, omit all reaction times which are 0 as these will manipulate the mean.\n",
    "\n",
    "- Rename \"congruence\" as \"accuracy\" and save the dataframe as a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We add a column \"congruence\".\n",
    "# The assign function applies an anonymous function to all rows and save the result by the name of the argument\n",
    "df_concatenated = df_concatenated.assign(congruence=lambda row: row.StimulusType == row.response)\n",
    "\n",
    "# We create a new dataframe with averaged data\n",
    "# Pandas works a bit like SQL:\n",
    "#   1. First we filter for invalid reaction times ...\n",
    "#   2. ... than we group the data by our columns of interest ...\n",
    "#   3. ... and calculate the average.\n",
    "df_concatenated_avg = df_concatenated[df_concatenated.RT > 0]\\\n",
    "    .groupby(['SubjectID', 'StimulusType'])\\\n",
    "    .mean()\n",
    "\n",
    "# We rename \"congruence\" to \"accuracy\" inplace.\n",
    "df_concatenated_avg.rename(index=str, columns={\"congruence\": \"accuracy\"}, inplace=True)\n",
    "\n",
    "# Afterwards, we save the result into the already created folder.\n",
    "DATAPATH = Path.cwd() / 'Processed' / 'data_concatenated_averaged.csv'\n",
    "with DATAPATH.open(mode='w', newline='') as file:\n",
    "    df_concatenated_avg.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Assignment 2: Statistical plotting [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### a) Boxplot and Violinplot [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Plot the RT of each trial for all subjects as a stripplot and a boxplot on top of each other. Do the same with a striplot and a violinplot. Plot go trials as green dots and no-go trails as red dots. Reminder: don't forget to mask the data where RT=0. Make sure that the legends are informative (Don't display duplicated legends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "data_concat = pd.read_csv(os.getcwd() + \"/Processed/data_concatenated.csv\")\n",
    "\n",
    "# create two axes\n",
    "fig, axes = plt.subplots(nrows=1,ncols=2)\n",
    "\n",
    "# first subplot with stripplot and boxplot\n",
    "#stripplot with data from group 2\n",
    "sns.stripplot(data = data_concat.drop(data_concat[data_concat.RT == 0].index), \n",
    "              x='SubjectID', y='RT',\n",
    "              hue='StimulusType',palette=[\"r\", \"g\"], \n",
    "              ax=axes[0])\n",
    "#boxplot\n",
    "ax1 = sns.boxplot(data = data_concat.drop(data_concat[data_concat.RT == 0].index), \n",
    "                  x='SubjectID', y='RT', color='0.9',\n",
    "                  ax=axes[0])\n",
    "\n",
    "# second subplot with stripplot and violinplot\n",
    "#stripplot\n",
    "sns.stripplot(data = data_concat.drop(data_concat[data_concat.RT == 0].index), \n",
    "              x='SubjectID', y='RT', \n",
    "              hue='StimulusType',palette=[\"r\", \"g\"],\n",
    "              ax=axes[1])\n",
    "#violinplot\n",
    "ax2 = sns.violinplot(data = data_concat.drop(data_concat[data_concat.RT == 0].index), \n",
    "                     x='SubjectID', y='RT', \n",
    "                     hue='StimulusType',palette=[\"r\", \"g\"], split=True, \n",
    "                     ax=axes[1])\n",
    "\n",
    "# handling legends\n",
    "#only keep one label to avoid duplicates\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "ax1.legend(handles, labels[0:2])\n",
    "\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "ax2.legend(handles, labels[0:2])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### b) Violinplot combining all data of all groups [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "- Make a dataframe consisting of all data across groups. You already did this in 1.a). At the end this dataframe you should have 8 * 11 * 100 rows.\n",
    "\n",
    "- Every group has used their ID convention. Make sure that every data point follows this SubjectID system: group number + \"00\" + subject number.  \n",
    "e.g) 3002 for the second subject of the third group.\n",
    "\n",
    "- Compute average RT and accuaracy for each subject in the big dataframe you just created. You already did this in 1.b). At the end this dataframe will have 8 * 11 rows.\n",
    "\n",
    "- On the first column plot average RT and accuracy for 8 subjects from your group's data. Use violinplot and split go/no-go conditions.\n",
    "\n",
    "- On the second column plot average RT and accuracy for 80 subjects from all data. Use violinplot and split go/no-go conditions.\n",
    "\n",
    "- Do you see any difference between the first column and the second column? What does this tell us about the central limit theorem (CLT) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# again create a concatenated dataframe over all (averaged) groups.\n",
    "# Don't forget to modify the Subject ID\n",
    "# TODO\n",
    "\n",
    "# Now it's time to plot your results\n",
    "figs, axes = plt.subplots(nrows=2, ncols=2, sharey=\"row\")\n",
    "\n",
    "# violin plot for your group's data\n",
    "# TODO\n",
    "\n",
    "# violin plot of all group's data\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Compare two datasets and relate it with CLT. Write your opinion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### c) Scatterplot [1 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Make a scatterplot comparing RT and accuracy. Do you see some correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
